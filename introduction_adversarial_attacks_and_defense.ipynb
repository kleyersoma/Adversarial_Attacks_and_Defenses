{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align:center; font-size:42px;'>Introduction to Adversarial Attacks and Defenses</h1>\n",
    "<img src='images/color_noise.jpg'/>\n",
    "<p>\n",
    "    <br>\n",
    "    Image classification is one of the principal paradigms of Computer Vision, a single misclassification can lead to disastrous events, for example a prediction error in a computer vision system for cancer prediction can yield into <a href='https://en.wikipedia.org/wiki/Type_I_and_type_I_errors#Type_I_error'>Type I</a>  or <a href='https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error'>Type II</a> kind of errors.\n",
    "    <br>\n",
    "    <br>\n",
    "    <a href='https://openai.com/blog/adversarial-example-research/'>Adversarial Attacks</a> are a technique of creating imperceptible changes to an image that can cause Computer Visions systems misclassify an image consistently.\n",
    "    <br>\n",
    "    <br>\n",
    "    This notebook will give a brief explanation and introduction to how implement Adversarial Attacks and how it is possible to defend Computer Visions models against these attacks.\n",
    "    <br>\n",
    "    <br>\n",
    "    <i>Note: This implementation will be done through <b>Tensorflow 2.0</b> and will not work for any version below 2.0.</i>\n",
    "    <h2>Math for Adversarial Attakcs</h2>\n",
    "    <br>\n",
    "    <a href='https://arxiv.org/abs/1412.6572'>Explaining and Harnessing Adversarial Examples</a> is a paper that describes a function known as <b>Fast Gradient Sign Method</b> or <b>FGSM</b> for generating adversarial noise, this function it is describe below:\n",
    "    <br>\n",
    "    <br>\n",
    "    <div style='text-align:center;font-size:20px;'>$η=ϵ sign(∇ₓ J(θ,x,y))$</div>\n",
    "    <br>\n",
    "    <br>\n",
    "    The correct resolution of this equation can be achieved by implmenting the next steps:\n",
    "    <ol>\n",
    "        <li>Take the gradient of the cost function with respect to the model's input, output and weights. This operations is expressed as:\n",
    "            <br>\n",
    "            <br>\n",
    "            <div style='text-align:center;font-size:16px;'>$∇ₓJ(θ, x, y)$</div>\n",
    "            <br>\n",
    "            The mathematical loss representation of the model is $J(θ, x, y)$, where $θ$ is the parameter of the model, $x$ is the input to the model and $y$ is the output/prediction of the model.</li>\n",
    "        <li>By taking the $sign$ of each term in the gradient, reducing it ot a matrix of $-1s$, $0s$ and $1s$.</li>\n",
    "        <li>Scaling down the perturbations by multiplying them, by some small float, $ϵ$.</li>\n",
    "    </ol>\n",
    "    This steps will result in $η$ which is the variable used to represent the adversarial perturbations.\n",
    "    <br>\n",
    "    <h2>Attack against a Computer Visions Neural Network Architecture</h2>\n",
    "    <br>\n",
    "    This method of attack will work for <code>cifar10</code> and <code>cifar100</code> or any dataset, the only change required would be to the structure of the model.\n",
    "    <br>\n",
    "    <br>\n",
    "    First it is needed to import the necessary libraries and packages.\n",
    "    <h3>Import Packages&Libraries</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist, cifar10, cifar100\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    It is necessary to use <code>tensorflow.keras</code> instead of <code>keras</code>, this because importing the module Keras through <code> import keras</code> will not work with Tensorflow 2.0, therefore it is necessary to import the Keras API designed into Tensorflow 2.0, for a better explanation of why this method of implementing Keras with Tensorflow 2.0 is required, <a href='https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/'>Phd. Adrian Rosebrock</a> has an excellent post explaining why.\n",
    "    <br>\n",
    "    <br>\n",
    "    <div><b>Note</b>: In case of this error during the fitting process:\n",
    "    <br>\n",
    "    <div style='padding:10px;margin:10px;background-color:pink' >UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
    "\t [[node sequential_3/conv2d_9/Conv2D (defined at /home/kleyer/anaconda3/envs/ai/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_2144]\n",
    "        <br>\n",
    "        <br>\n",
    "Function call stack:\n",
    "distributed_function</div></div>\n",
    "It can be fixed by running the below cell:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = tf.compat.v1.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Specify Labels</h3>\n",
    "<p>\n",
    "    Specifyin the word labels for the different classes the model will learn to differentiate. This is mostly redundant for MNIST, but it is a good practice to have it in for a smoother transition to other image classification datasets.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['zero',\n",
    "          'one',\n",
    "          'two',\n",
    "          'three',\n",
    "          'four',\n",
    "          'five',\n",
    "          'six',\n",
    "          'seven',\n",
    "          'eight',\n",
    "          'nine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>\n",
    "<p>\n",
    "    Set up number of classes to classify, quantity of rows and columns and kind of channel.\n",
    "    <br>\n",
    "    Steps for preprocessing data:\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Process Data.</li>\n",
    "        <li>Standardization.</li>\n",
    "        <li>Reshape.</li>\n",
    "        <li>Label One-Hot-Encoding.</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train set shape of (60000, 28, 28)\n",
      "X_test set shape of (10000, 28, 28)\n",
      "y_train set shape of (60000,)\n",
      "y_test set shape of (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train set shape of {}'.format(X_train.shape))\n",
    "print('X_test set shape of {}'.format(X_test.shape))\n",
    "print('y_train set shape of {}'.format(y_train.shape))\n",
    "print('y_test set shape of {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train set shape of (60000, 28, 28, 1)\n",
      "X_test set shape of (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape((-1, img_rows, img_cols, channels))\n",
    "X_test = X_test.reshape((-1, img_rows, img_cols, channels))\n",
    "\n",
    "print('X_train set shape of {}'.format(X_train.shape))\n",
    "print('X_test set shape of {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train set shape of (60000, 10)\n",
      "y_test set shape of (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('y_train set shape of {}'.format(y_train.shape))\n",
    "print('y_test set shape of {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a Validation Set</h3>\n",
    "<br>\n",
    "This validation set will be used for meassuring how well it is the network learning during its training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train set shape of (54000, 28, 28, 1)\n",
      "X_val set shape of (6000, 28, 28, 1)\n",
      "y_train set shape of (54000, 10)\n",
      "y_val set shape of (6000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train set shape of {}'.format(X_train.shape))\n",
    "print('X_val set shape of {}'.format(X_val.shape))\n",
    "print('y_train set shape of {}'.format(y_train.shape))\n",
    "print('y_val set shape of {}'.format(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define The Architecture of the Model</h3>\n",
    "<p>\n",
    "    This architecture design it is \"<i>in the fly</i>\" design, the reader can feel free to implement its own design.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_rows, img_cols, channels):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu', input_shape=(img_rows, img_cols, channels)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fitting the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/32\n",
      "54000/54000 [==============================] - 10s 186us/sample - loss: 0.0208 - accuracy: 0.8560 - val_loss: 0.0063 - val_accuracy: 0.9603\n",
      "Epoch 2/32\n",
      "54000/54000 [==============================] - 7s 130us/sample - loss: 0.0094 - accuracy: 0.9383 - val_loss: 0.0051 - val_accuracy: 0.9655\n",
      "Epoch 3/32\n",
      "54000/54000 [==============================] - 7s 125us/sample - loss: 0.0075 - accuracy: 0.9519 - val_loss: 0.0043 - val_accuracy: 0.9710\n",
      "Epoch 4/32\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 0.0064 - accuracy: 0.9587 - val_loss: 0.0040 - val_accuracy: 0.9735\n",
      "Epoch 5/32\n",
      "54000/54000 [==============================] - 7s 133us/sample - loss: 0.0058 - accuracy: 0.9630 - val_loss: 0.0039 - val_accuracy: 0.9747\n",
      "Epoch 6/32\n",
      "54000/54000 [==============================] - 7s 132us/sample - loss: 0.0055 - accuracy: 0.9647 - val_loss: 0.0035 - val_accuracy: 0.9768\n",
      "Epoch 7/32\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 0.0050 - accuracy: 0.9675 - val_loss: 0.0035 - val_accuracy: 0.9777\n",
      "Epoch 8/32\n",
      "54000/54000 [==============================] - 7s 126us/sample - loss: 0.0048 - accuracy: 0.9691 - val_loss: 0.0038 - val_accuracy: 0.9762\n",
      "Epoch 9/32\n",
      "54000/54000 [==============================] - 7s 131us/sample - loss: 0.0043 - accuracy: 0.9726 - val_loss: 0.0030 - val_accuracy: 0.9815\n",
      "Epoch 10/32\n",
      "54000/54000 [==============================] - 7s 126us/sample - loss: 0.0042 - accuracy: 0.9733 - val_loss: 0.0033 - val_accuracy: 0.9792\n",
      "Epoch 11/32\n",
      "54000/54000 [==============================] - 7s 132us/sample - loss: 0.0040 - accuracy: 0.9746 - val_loss: 0.0032 - val_accuracy: 0.9807\n",
      "Epoch 12/32\n",
      "54000/54000 [==============================] - 7s 129us/sample - loss: 0.0039 - accuracy: 0.9755 - val_loss: 0.0030 - val_accuracy: 0.9812\n",
      "Epoch 13/32\n",
      "54000/54000 [==============================] - 8s 146us/sample - loss: 0.0039 - accuracy: 0.9754 - val_loss: 0.0035 - val_accuracy: 0.9777\n",
      "Epoch 14/32\n",
      "54000/54000 [==============================] - 7s 132us/sample - loss: 0.0037 - accuracy: 0.9764 - val_loss: 0.0031 - val_accuracy: 0.9810\n",
      "Epoch 15/32\n",
      "54000/54000 [==============================] - 6s 115us/sample - loss: 0.0035 - accuracy: 0.9778 - val_loss: 0.0029 - val_accuracy: 0.9825\n",
      "Epoch 16/32\n",
      "54000/54000 [==============================] - 6s 111us/sample - loss: 0.0034 - accuracy: 0.9787 - val_loss: 0.0033 - val_accuracy: 0.9798\n",
      "Epoch 17/32\n",
      "54000/54000 [==============================] - 8s 148us/sample - loss: 0.0032 - accuracy: 0.9799 - val_loss: 0.0032 - val_accuracy: 0.9803\n",
      "Epoch 18/32\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 0.0034 - accuracy: 0.9785 - val_loss: 0.0032 - val_accuracy: 0.9798\n",
      "Epoch 19/32\n",
      "54000/54000 [==============================] - 6s 117us/sample - loss: 0.0032 - accuracy: 0.9805 - val_loss: 0.0029 - val_accuracy: 0.9818\n",
      "Epoch 20/32\n",
      "54000/54000 [==============================] - 7s 123us/sample - loss: 0.0031 - accuracy: 0.9803 - val_loss: 0.0031 - val_accuracy: 0.9810\n",
      "Epoch 21/32\n",
      "54000/54000 [==============================] - 7s 129us/sample - loss: 0.0030 - accuracy: 0.9812 - val_loss: 0.0030 - val_accuracy: 0.9818\n",
      "Epoch 22/32\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 0.0030 - accuracy: 0.9815 - val_loss: 0.0029 - val_accuracy: 0.9817\n",
      "Epoch 23/32\n",
      "54000/54000 [==============================] - 7s 133us/sample - loss: 0.0029 - accuracy: 0.9820 - val_loss: 0.0031 - val_accuracy: 0.9820\n",
      "Epoch 24/32\n",
      "54000/54000 [==============================] - 7s 133us/sample - loss: 0.0029 - accuracy: 0.9826 - val_loss: 0.0032 - val_accuracy: 0.9805\n",
      "Epoch 25/32\n",
      "54000/54000 [==============================] - 6s 119us/sample - loss: 0.0029 - accuracy: 0.9823 - val_loss: 0.0032 - val_accuracy: 0.9813\n",
      "Epoch 26/32\n",
      "54000/54000 [==============================] - 5s 102us/sample - loss: 0.0028 - accuracy: 0.9826 - val_loss: 0.0033 - val_accuracy: 0.9810\n",
      "Epoch 27/32\n",
      "54000/54000 [==============================] - 6s 118us/sample - loss: 0.0027 - accuracy: 0.9833 - val_loss: 0.0030 - val_accuracy: 0.9805\n",
      "Epoch 28/32\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 0.0027 - accuracy: 0.9833 - val_loss: 0.0034 - val_accuracy: 0.9800\n",
      "Epoch 29/32\n",
      "54000/54000 [==============================] - 7s 121us/sample - loss: 0.0026 - accuracy: 0.9841 - val_loss: 0.0030 - val_accuracy: 0.9825\n",
      "Epoch 30/32\n",
      "54000/54000 [==============================] - 7s 132us/sample - loss: 0.0027 - accuracy: 0.9837 - val_loss: 0.0028 - val_accuracy: 0.9837\n",
      "Epoch 31/32\n",
      "54000/54000 [==============================] - 6s 117us/sample - loss: 0.0026 - accuracy: 0.9843 - val_loss: 0.0031 - val_accuracy: 0.9822\n",
      "Epoch 32/32\n",
      "54000/54000 [==============================] - 7s 127us/sample - loss: 0.0027 - accuracy: 0.9835 - val_loss: 0.0034 - val_accuracy: 0.9803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f367812ea20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "          y_train,\n",
    "         batch_size=32,\n",
    "         epochs=32,\n",
    "         validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Training Performance Evaluation</h3>\n",
    "<p>\n",
    "    Evaluate the baseline accuracy with:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on regular images:  [0.003343778435196667, 0.981]\n"
     ]
    }
   ],
   "source": [
    "print('Base accuracy on regular images: ', model.evaluate(X_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The model accuracy is <i>98.08%</i>\n",
    "    <br>\n",
    "    <h3>Generating Perturbations</h3>\n",
    "    <br>\n",
    "    This perturbations will be generated through a function, <code>adversarial_pattern(image, label)</code> that it will take an image and it is going to apply perturbations to it, these perturbations is just noise, this function will also take the correct label that classifies the image.\n",
    "    <br>\n",
    "    <h4>Describing the work of the Function <code>adversarial_pattern(image, label)</code></h4>\n",
    "    <ol>\n",
    "        <li>Converts the image into a tensor with <code>tf.cast(image, tf.float32)</code>.</li>\n",
    "        <li>Gradients are set up for calculating them. This is done through the object <code>tf.GradientTape()</code>. The <code>GradientTape</code> object keeps tarck of what happens to a tensor that it's observing and can automatically calculate the gradients for it.\n",
    "    <br>\n",
    "    <br>\n",
    "            This is represented by the mathematically function $J(θ, x, y)$, as $J$ is the loss between the input image $x$ and the model predictions $y$.\n",
    "    <br></li>\n",
    "    <li>Gradients are now able to be calculated with <code>tf.GradientTape().tape.gradient(loss, image)</code>.\n",
    "    <br>\n",
    "    <br>\n",
    "        Calculating the gradient of the cost function is represented in the equation by $∇ₓ$.</li>\n",
    "        <li>Now it is possible to take the $sign$ of the equation through <code>tf.sign(gradient)</code></li>\n",
    "    <li>For last it is possible to return the value of the $sign$.</li>\n",
    "    </ol>\n",
    "    The below cell has the implementation of the <code>adversarial_pattern(image, label)</code> function.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_pattern(image, label):\n",
    "    # Step 1\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Step 2\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        # The loss function has to be the same that the Neural Network was trained with.\n",
    "        loss = tf.keras.losses.MSE(label, prediction)\n",
    "    \n",
    "    # Step 3\n",
    "    gradient = tape.gradient(loss, image)\n",
    "    \n",
    "    # Step 4\n",
    "    signed_grad = tf.sign(gradient)\n",
    "    \n",
    "    # Step 5\n",
    "    return signed_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example Adversarial Image</h3>\n",
    "<p>\n",
    "    The function <code>adversarial_pattern(image, label)</code> is going to be demostrated.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regular image from the train set will be used with its corresponding label\n",
    "image = X_train[10]\n",
    "image_label = y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the adversarial perturbations\n",
    "perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The <code>.numpy()</code> at the end of the line converts the EagerTensor object into a numpy array.\n",
    "    <br>\n",
    "    <br>\n",
    "    Now it is time to apply the perturbations. When multiplying the perturbations by <code>0.1</code> reduces the effect of the perturbations, this makes the perturbations less noticeable, the quantity <code>0.1</code> is represented mathematically by $ϵ$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial = image + perturbations * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the adversarial attack has been performed it is time to see if it affects the Neural Network predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true label was: seven\n",
      "The prediction after the attack is: nine\n"
     ]
    }
   ],
   "source": [
    "print('The true label was: {}'.format(labels[model.predict(image.reshape((1, img_rows, img_cols, channels))).argmax()]))\n",
    "print('The prediction after the attack is: {}'.format(labels[model.predict(adversarial).argmax()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "   And yes, it does affect the Neural Network prediction, but how efficient it is the attack? but first the image before and after the attack is going to be plotted, this will demostrated how hard it is to trick the Neural Network.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if channels == 1:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols)))\n",
    "else:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols, channels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f36fe4eab00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.reshape((img_rows, img_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    It is clearly to see the difference between the space that does not represent the number\n",
    "    <h3>Adversarial Attack Data Generator</h3>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Defending Against Adversarial Attacks</h2>\n",
    "<p>\n",
    "    When a model is deployed into the real world to make decisions, it has to be able to defend itself against Adversarial Attacks, that may cause error predictions, there are several ways in which a model can defend itself against this type of attacks, the one that is going to be implemented in this notebook is just to add adversarial saamples in the training set and train the network with that set.\n",
    "    <br>\n",
    "    <h3>Building an Adversarial Example Generator</h3>\n",
    "    <br>\n",
    "    This training will be done inside of a generator function, <code>generator_adversarial_samples(batchsize, X_set, y_set)</code> this function will allow to directly fit the model on the generator of images and train the model with those generated images, it has a parameter for <code>batchsize</code> of the images.\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Describing the work of the Function <code>generator_adversarial_samples(batchsize, X_set, y_set)</code></b>\n",
    "    <ol>\n",
    "        <li>The generator takes an argument for the <code>batchsize</code> which it is used when fitting directly on the generator for generating different <code>batchsize</code> samples.</li>\n",
    "    <li>Enters a loop and set up the <code>x</code> and <code>y</code> variables that will hold the adversarial images and their corresponding labels.</li>\n",
    "    <li>For each <code>batchsize</code> it picks up the index of a random image.</li>\n",
    "        <li>Then it generates the perturbations.</li>\n",
    "        <li>The value of $ϵ$ is set up equal to some small float.</li>\n",
    "        <li>The perturbations are added to the images.</li>\n",
    "        <li>Therefore the adversarial images and its corresponding label are appended to a dictionary to hold them together.</li>\n",
    "        <li>After all these steps are done for every batch, the main loop is finished and the <code>x</code> and <code>y</code> arrays are <code>yielded</code>.</li>\n",
    "    </ol>\n",
    "    This functions is coded in the below cell:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1, it takes the batchsize argument\n",
    "def generate_adversarial_samples(batchsize, X_set, y_set):\n",
    "    # Step 2\n",
    "    while True:\n",
    "        x = []\n",
    "        y = []\n",
    "        # Step 3\n",
    "        for batch in range(batchsize):\n",
    "            N = len(y_set) - 1\n",
    "            N = random.randint(0, N)\n",
    "            label = y_set[N]\n",
    "            image = X_set[N]\n",
    "            \n",
    "            # Step 4\n",
    "            perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), label).numpy()\n",
    "            \n",
    "            # Step 5\n",
    "            epsilon = 0.1\n",
    "            \n",
    "            # Step 6\n",
    "            adversarial = image + perturbations * epsilon\n",
    "            \n",
    "            # Step 7\n",
    "            x.append(adversarial)\n",
    "            y.append(y_set[N])\n",
    "        \n",
    "        # Step 8.a\n",
    "        x = np.asarray(x).reshape((batchsize, img_rows, img_cols, channels))\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Step 8.b\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The <code>generator_adversarial_samples(batchsize, X_set, y_set)</code> function may take a considerable amount of time and the performance of the model can be drastically improved by allowing the perturbations to be calculated in parallel using multi-threading.\n",
    "    <br>\n",
    "    <h3>Evaluating the Baseline Model</h3>\n",
    "    <br>\n",
    "    Thanks to the capacities of the generator function, it is possible to generate as many images as it is necessary. Before starting the process of defending the model is good to have control to measure how effective the technique is, for this an adversarial test dataset is generated with the next line:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adversarial_test, y_adversarial_test = next(generate_adversarial_samples(10000, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The model has to be evaluated on the adversarial test set.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on adversarial images: [0.12750121088027955, 0.3042]\n"
     ]
    }
   ],
   "source": [
    "print('Base accuracy on adversarial images: {}'.format(model.evaluate(X_adversarial_test, y_adversarial_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The model has achivied an accuracy of <b>30.42%</b>, when $ϵ=0.1$.\n",
    "    <h3>Training a Defended Model</h3>\n",
    "    <br>\n",
    "    There are two options; the model can be train directly on the generator or pre-generate a dataset of adversarial images to fit on.\n",
    "\n",
    "Training the model on a generator allows new images to be made on-the-fly to fool the model, which would mean that the model would learn to adapt to more complex images, and become more robust in the process. Despite these benefits, this method requires significantly more computing power, and for the sake of this tutorial, we are going to pre-generate a dataset of adversarial images.\n",
    "\n",
    "   This training adversarial set is formed by <i>54000</i> images, the same quantity of normal images presented in the training normal set, (the quantity of images could be defined empirically, but if it is bigger thant the quantity of normal images can lead to an imbalanced dataset, this if both sets are merged into one, this merged will be implemented later), if the quantity the images is to low the model will not learn properly and if it is to high, the computational resources may not be enough to perform the generation of images, the computer may run out of memory.\n",
    "    After the images are generated, it is time to train the model with the adversarial set\n",
    "\n",
    "New training images wiht nearly the same line we used to generate the testing images:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adversarial_train, y_adversarial_train = next(generate_adversarial_samples(54000, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Optional Save&Load the Normal and Adversarial Sets</h4>\n",
    "<p>\n",
    "    If it is needed to save the adversarial sets or the normal sets, the cell below helps for that.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train.npy', X_train)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_test.npy', y_test)\n",
    "\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('y_val.npy', y_val)\n",
    "\n",
    "np.save('X_adversarial_train.npy', X_adversarial_train)\n",
    "np.save('y_adversarial_train.npy', y_adversarial_train)\n",
    "\n",
    "np.save('X_adversarial_test.npy', X_adversarial_test)\n",
    "np.save('y_adversarial_test.npy', y_adversarial_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    To load the sets:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "X_val = np.load('X_val.npy')\n",
    "y_val = np.load('y_val.npy')\n",
    "\n",
    "X_adversarial_train = np.load('X_adversarial_train.npy')\n",
    "y_adversarial_train = np.load('y_adversarial_train.npy')\n",
    "\n",
    "X_adversarial_test = np.load('X_adversarial_test.npy')\n",
    "y_adversarial_test = np.load('y_adversarial_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fit the Model with the Adversarial Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/32\n",
      "54000/54000 [==============================] - 6s 109us/sample - loss: 0.0150 - accuracy: 0.9187 - val_loss: 0.0059 - val_accuracy: 0.9682\n",
      "Epoch 2/32\n",
      "54000/54000 [==============================] - 7s 124us/sample - loss: 0.0045 - accuracy: 0.9746 - val_loss: 0.0046 - val_accuracy: 0.9742\n",
      "Epoch 3/32\n",
      "54000/54000 [==============================] - 7s 121us/sample - loss: 0.0034 - accuracy: 0.9806 - val_loss: 0.0053 - val_accuracy: 0.9700\n",
      "Epoch 4/32\n",
      "54000/54000 [==============================] - 6s 118us/sample - loss: 0.0027 - accuracy: 0.9846 - val_loss: 0.0052 - val_accuracy: 0.9712\n",
      "Epoch 5/32\n",
      "54000/54000 [==============================] - 7s 126us/sample - loss: 0.0024 - accuracy: 0.9865 - val_loss: 0.0055 - val_accuracy: 0.9697\n",
      "Epoch 6/32\n",
      "54000/54000 [==============================] - 6s 109us/sample - loss: 0.0023 - accuracy: 0.9873 - val_loss: 0.0053 - val_accuracy: 0.9707\n",
      "Epoch 7/32\n",
      "54000/54000 [==============================] - 7s 125us/sample - loss: 0.0021 - accuracy: 0.9884 - val_loss: 0.0051 - val_accuracy: 0.9720\n",
      "Epoch 8/32\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 0.0020 - accuracy: 0.9890 - val_loss: 0.0060 - val_accuracy: 0.9677\n",
      "Epoch 9/32\n",
      "54000/54000 [==============================] - 7s 127us/sample - loss: 0.0018 - accuracy: 0.9901 - val_loss: 0.0064 - val_accuracy: 0.9645\n",
      "Epoch 10/32\n",
      "54000/54000 [==============================] - 7s 129us/sample - loss: 0.0019 - accuracy: 0.9898 - val_loss: 0.0065 - val_accuracy: 0.9658\n",
      "Epoch 11/32\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 0.0017 - accuracy: 0.9907 - val_loss: 0.0067 - val_accuracy: 0.9635\n",
      "Epoch 12/32\n",
      "54000/54000 [==============================] - 7s 125us/sample - loss: 0.0019 - accuracy: 0.9894 - val_loss: 0.0063 - val_accuracy: 0.9668\n",
      "Epoch 13/32\n",
      "54000/54000 [==============================] - 5s 102us/sample - loss: 0.0017 - accuracy: 0.9905 - val_loss: 0.0064 - val_accuracy: 0.9657\n",
      "Epoch 14/32\n",
      "54000/54000 [==============================] - 7s 127us/sample - loss: 0.0017 - accuracy: 0.9907 - val_loss: 0.0059 - val_accuracy: 0.9693\n",
      "Epoch 15/32\n",
      "54000/54000 [==============================] - 5s 93us/sample - loss: 0.0018 - accuracy: 0.9901 - val_loss: 0.0063 - val_accuracy: 0.9667\n",
      "Epoch 16/32\n",
      "54000/54000 [==============================] - 6s 109us/sample - loss: 0.0016 - accuracy: 0.9916 - val_loss: 0.0068 - val_accuracy: 0.9632\n",
      "Epoch 17/32\n",
      "54000/54000 [==============================] - 6s 110us/sample - loss: 0.0019 - accuracy: 0.9901 - val_loss: 0.0078 - val_accuracy: 0.9587\n",
      "Epoch 18/32\n",
      "54000/54000 [==============================] - 6s 103us/sample - loss: 0.0018 - accuracy: 0.9903 - val_loss: 0.0071 - val_accuracy: 0.9635\n",
      "Epoch 19/32\n",
      "54000/54000 [==============================] - 5s 88us/sample - loss: 0.0019 - accuracy: 0.9897 - val_loss: 0.0077 - val_accuracy: 0.9597\n",
      "Epoch 20/32\n",
      "54000/54000 [==============================] - 6s 106us/sample - loss: 0.0022 - accuracy: 0.9887 - val_loss: 0.0078 - val_accuracy: 0.9597\n",
      "Epoch 21/32\n",
      "54000/54000 [==============================] - 5s 100us/sample - loss: 0.0019 - accuracy: 0.9901 - val_loss: 0.0104 - val_accuracy: 0.9458\n",
      "Epoch 22/32\n",
      "54000/54000 [==============================] - 6s 116us/sample - loss: 0.0018 - accuracy: 0.9908 - val_loss: 0.0078 - val_accuracy: 0.9592\n",
      "Epoch 23/32\n",
      "54000/54000 [==============================] - 5s 99us/sample - loss: 0.0020 - accuracy: 0.9896 - val_loss: 0.0070 - val_accuracy: 0.9635\n",
      "Epoch 24/32\n",
      "54000/54000 [==============================] - 5s 100us/sample - loss: 0.0021 - accuracy: 0.9893 - val_loss: 0.0075 - val_accuracy: 0.9610\n",
      "Epoch 25/32\n",
      "54000/54000 [==============================] - 8s 150us/sample - loss: 0.0019 - accuracy: 0.9902 - val_loss: 0.0065 - val_accuracy: 0.9663\n",
      "Epoch 26/32\n",
      "54000/54000 [==============================] - 6s 118us/sample - loss: 0.0021 - accuracy: 0.9893 - val_loss: 0.0068 - val_accuracy: 0.9650\n",
      "Epoch 27/32\n",
      "54000/54000 [==============================] - 7s 130us/sample - loss: 0.0025 - accuracy: 0.9870 - val_loss: 0.0083 - val_accuracy: 0.9580\n",
      "Epoch 28/32\n",
      "54000/54000 [==============================] - 7s 135us/sample - loss: 0.0025 - accuracy: 0.9874 - val_loss: 0.0097 - val_accuracy: 0.9508\n",
      "Epoch 29/32\n",
      "54000/54000 [==============================] - 8s 147us/sample - loss: 0.0024 - accuracy: 0.9876 - val_loss: 0.0107 - val_accuracy: 0.9450\n",
      "Epoch 30/32\n",
      "54000/54000 [==============================] - 6s 114us/sample - loss: 0.0024 - accuracy: 0.9875 - val_loss: 0.0092 - val_accuracy: 0.9527\n",
      "Epoch 31/32\n",
      "54000/54000 [==============================] - 7s 126us/sample - loss: 0.0022 - accuracy: 0.9890 - val_loss: 0.0096 - val_accuracy: 0.9507\n",
      "Epoch 32/32\n",
      "54000/54000 [==============================] - 5s 101us/sample - loss: 0.0020 - accuracy: 0.9896 - val_loss: 0.0086 - val_accuracy: 0.9560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3668762ba8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_adversarial_train,\n",
    "          y_adversarial_train,\n",
    "          batch_size=32,\n",
    "          epochs=32,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Once the model has been train with the train adversarial set, it is time to evaluate the performance of the model trained with that set.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defended accuracy on adversarial images: [0.0015206410317618803, 0.9922]\n",
      "Defended accuracy on regular images: [0.008219032578776334, 0.9579]\n"
     ]
    }
   ],
   "source": [
    "print('Defended accuracy on adversarial images: {}' .format(model.evaluate(X_adversarial_test, y_adversarial_test, verbose=0)))\n",
    "print('Defended accuracy on regular images: {}' .format(model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The model reaches an accuracy of <b>99.22%</b> on predicting adversarial images and the accuracy on normal images it is of <b>95.79%</b>, it is lower this can be explained by the imbalanced in the dataset, <i>the model was never trained with normal images, therefore can not classify them as well as adversarial images</i>. \n",
    "</p>\n",
    "<h3>Joining Training Sets</h3>\n",
    "<p>\n",
    "    This part is done only to see if the performance of the model improves, the normal train set is going to be added to the adversarial set.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.row_stack((X_adversarial_train, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108000, 28, 28, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.row_stack((y_adversarial_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108000, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that the training sets have been jointed, it is time to train the Neural Network and see if its performance increases.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108000 samples, validate on 6000 samples\n",
      "Epoch 1/32\n",
      "108000/108000 [==============================] - 14s 128us/sample - loss: 0.0075 - accuracy: 0.9619 - val_loss: 0.0093 - val_accuracy: 0.9532\n",
      "Epoch 2/32\n",
      "108000/108000 [==============================] - 12s 113us/sample - loss: 0.0061 - accuracy: 0.9690 - val_loss: 0.0058 - val_accuracy: 0.9708\n",
      "Epoch 3/32\n",
      "108000/108000 [==============================] - 14s 128us/sample - loss: 0.0060 - accuracy: 0.9693 - val_loss: 0.0061 - val_accuracy: 0.9692\n",
      "Epoch 4/32\n",
      "108000/108000 [==============================] - 13s 120us/sample - loss: 0.0062 - accuracy: 0.9686 - val_loss: 0.0074 - val_accuracy: 0.9625\n",
      "Epoch 5/32\n",
      "108000/108000 [==============================] - 12s 109us/sample - loss: 0.0060 - accuracy: 0.9695 - val_loss: 0.0070 - val_accuracy: 0.9647\n",
      "Epoch 6/32\n",
      "108000/108000 [==============================] - 13s 124us/sample - loss: 0.0064 - accuracy: 0.9675 - val_loss: 0.0067 - val_accuracy: 0.9662\n",
      "Epoch 7/32\n",
      "108000/108000 [==============================] - 13s 119us/sample - loss: 0.0059 - accuracy: 0.9704 - val_loss: 0.0066 - val_accuracy: 0.9670\n",
      "Epoch 8/32\n",
      "108000/108000 [==============================] - 14s 129us/sample - loss: 0.0059 - accuracy: 0.9705 - val_loss: 0.0071 - val_accuracy: 0.9637\n",
      "Epoch 9/32\n",
      "108000/108000 [==============================] - 12s 110us/sample - loss: 0.0063 - accuracy: 0.9683 - val_loss: 0.0080 - val_accuracy: 0.9595\n",
      "Epoch 10/32\n",
      "108000/108000 [==============================] - 9s 80us/sample - loss: 0.0061 - accuracy: 0.9691 - val_loss: 0.0070 - val_accuracy: 0.9647\n",
      "Epoch 11/32\n",
      "108000/108000 [==============================] - 9s 82us/sample - loss: 0.0064 - accuracy: 0.9675 - val_loss: 0.0063 - val_accuracy: 0.9685\n",
      "Epoch 12/32\n",
      "108000/108000 [==============================] - 8s 75us/sample - loss: 0.0059 - accuracy: 0.9701 - val_loss: 0.0057 - val_accuracy: 0.9713\n",
      "Epoch 13/32\n",
      "108000/108000 [==============================] - 8s 77us/sample - loss: 0.0067 - accuracy: 0.9663 - val_loss: 0.0067 - val_accuracy: 0.9665\n",
      "Epoch 14/32\n",
      "108000/108000 [==============================] - 11s 100us/sample - loss: 0.0075 - accuracy: 0.9625 - val_loss: 0.0061 - val_accuracy: 0.9692\n",
      "Epoch 15/32\n",
      "108000/108000 [==============================] - 10s 94us/sample - loss: 0.0071 - accuracy: 0.9644 - val_loss: 0.0063 - val_accuracy: 0.9685\n",
      "Epoch 16/32\n",
      "108000/108000 [==============================] - 9s 87us/sample - loss: 0.0069 - accuracy: 0.9651 - val_loss: 0.0090 - val_accuracy: 0.9547\n",
      "Epoch 17/32\n",
      "108000/108000 [==============================] - 11s 104us/sample - loss: 0.0074 - accuracy: 0.9630 - val_loss: 0.0073 - val_accuracy: 0.9633\n",
      "Epoch 18/32\n",
      "108000/108000 [==============================] - 9s 87us/sample - loss: 0.0070 - accuracy: 0.9649 - val_loss: 0.0075 - val_accuracy: 0.9620\n",
      "Epoch 19/32\n",
      "108000/108000 [==============================] - 9s 82us/sample - loss: 0.0074 - accuracy: 0.9627 - val_loss: 0.0078 - val_accuracy: 0.9608\n",
      "Epoch 20/32\n",
      "108000/108000 [==============================] - 8s 74us/sample - loss: 0.0075 - accuracy: 0.9625 - val_loss: 0.0100 - val_accuracy: 0.9497\n",
      "Epoch 21/32\n",
      "108000/108000 [==============================] - 10s 89us/sample - loss: 0.0078 - accuracy: 0.9607 - val_loss: 0.0076 - val_accuracy: 0.9620\n",
      "Epoch 22/32\n",
      "108000/108000 [==============================] - 9s 87us/sample - loss: 0.0072 - accuracy: 0.9638 - val_loss: 0.0079 - val_accuracy: 0.9602\n",
      "Epoch 23/32\n",
      "108000/108000 [==============================] - 9s 83us/sample - loss: 0.0074 - accuracy: 0.9630 - val_loss: 0.0097 - val_accuracy: 0.9513\n",
      "Epoch 24/32\n",
      "108000/108000 [==============================] - 8s 77us/sample - loss: 0.0077 - accuracy: 0.9615 - val_loss: 0.0093 - val_accuracy: 0.9532\n",
      "Epoch 25/32\n",
      "108000/108000 [==============================] - 10s 92us/sample - loss: 0.0070 - accuracy: 0.9650 - val_loss: 0.0063 - val_accuracy: 0.9683\n",
      "Epoch 26/32\n",
      "108000/108000 [==============================] - 8s 78us/sample - loss: 0.0073 - accuracy: 0.9634 - val_loss: 0.0074 - val_accuracy: 0.9627\n",
      "Epoch 27/32\n",
      "108000/108000 [==============================] - 12s 107us/sample - loss: 0.0071 - accuracy: 0.9646 - val_loss: 0.0072 - val_accuracy: 0.9640\n",
      "Epoch 28/32\n",
      "108000/108000 [==============================] - 11s 103us/sample - loss: 0.0075 - accuracy: 0.9622 - val_loss: 0.0067 - val_accuracy: 0.9667\n",
      "Epoch 29/32\n",
      "108000/108000 [==============================] - 10s 93us/sample - loss: 0.0079 - accuracy: 0.9606 - val_loss: 0.0080 - val_accuracy: 0.9600\n",
      "Epoch 30/32\n",
      "108000/108000 [==============================] - 10s 91us/sample - loss: 0.0072 - accuracy: 0.9641 - val_loss: 0.0062 - val_accuracy: 0.9688\n",
      "Epoch 31/32\n",
      "108000/108000 [==============================] - 11s 98us/sample - loss: 0.0070 - accuracy: 0.9652 - val_loss: 0.0074 - val_accuracy: 0.9627\n",
      "Epoch 32/32\n",
      "108000/108000 [==============================] - 11s 99us/sample - loss: 0.0080 - accuracy: 0.9599 - val_loss: 0.0070 - val_accuracy: 0.9650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f36ed6e5a20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=32,\n",
    "          epochs=32,\n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defended accuracy on adversarial images: [0.0021950245991959223, 0.989]\n",
      "Defended accuracy on regular images: [0.007384727578242857, 0.963]\n"
     ]
    }
   ],
   "source": [
    "print('Defended accuracy on adversarial images: {}' .format(model.evaluate(X_adversarial_test, y_adversarial_test, verbose=0)))\n",
    "print('Defended accuracy on regular images: {}' .format(model.evaluate(X_test, y_test, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The model performs slightly better detecting normal images but it does not detected adversarial images as well as the last model. Finally the normal test set of images will be merge into the test set of adversarial images just to see how well the model does.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_set = np.row_stack((X_adversarial_test, X_test))\n",
    "y_final_set = np.row_stack((y_adversarial_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 28, 28, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defended accuracy on regular and adversarial images: [0.0047898760852271285, 0.976]\n"
     ]
    }
   ],
   "source": [
    "print('Defended accuracy on regular and adversarial images: {}' .format(model.evaluate(X_final_set, y_final_set, verbose=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The model has an accuracy of <b>97.6%</b>.\n",
    "    <br>\n",
    "    <br>\n",
    "    <i>How can the model differentiate from adversarial images into normal images?</i> This question will be answered in another notebook.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2>\n",
    "<p>\n",
    "    Adversarial Attacks are presented as a corner stone to understand how Neural Networks for Computer Vision can be tricked with simple changes in the image that they have to inferred.\n",
    "    <br>\n",
    "    <br>\n",
    "    A new paper, <a href='https://arxiv.org/pdf/1905.02175.pdf'>Adversarial Examples Are Not Bugs, They Are Features</a> intends to explain why adversarial attacks trick these Computer Vision Neural Network, the authors from this paper argues that there are microscopic features in every dataset that humans are unable to perceive, but neural networks utilize to their fullest extent. Adversarial examples manipulate these features to throw off classifiers, and they can do so without much effect to human viewers.\n",
    "    <br>\n",
    "    <br>\n",
    "    When a Neural Network has been trained to be resistant to adversarial attacks, the Neural Network does not rely on these microscopic features, these causes two primary effects:\n",
    "    <ol>\n",
    "        <li>The Neural Network becomes more robust and uses what humans consider to be the actual image more in its predictions.</li>\n",
    "        <li>As the Neural Network do not understand these imperceptible pixels to help its prediction, its accuracy decreases as it is now given a harder task.</li>\n",
    "    </ol>\n",
    "    <br>\n",
    "    These kind of attacks can be intentionally and non-intentionally implemented and the risk of generating wrong predictions increases, for example an intentional attack for tricking a Computer Vision Neural Network for autonomous car can cause an mortal accident.\n",
    "    <br>\n",
    "    <br>\n",
    "    On the topic of Adversarial Attacks there are black-box vs white-box attacks, new methods for generating perturbations, this notebook implemented a white-box attack with one of the simple techniques for perturbations.\n",
    "    <br>\n",
    "    <br>\n",
    "    To understand these attacks it is the only way in which we can prepared Neural Network to be robust against them.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
