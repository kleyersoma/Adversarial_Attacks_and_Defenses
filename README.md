# Adversarial Attacks and Defenses

***Introduction to Adversarial Attacks to Computer Vision Neural Networks and How to Defend from It***

Image classification is one of the principal paradigms of Computer Vision, a single misclassification can lead to disastrous events, 
for example a prediction error in a computer vision system for cancer prediction can yield into Type I or Type II kind of errors.

Adversarial Attacks are a technique of creating imperceptible changes to an image that can cause Computer Visions systems 
misclassify an image consistently.

This notebook will give a brief explanation and introduction to how implement Adversarial Attacks and how it is possible to defend 
Computer Visions models against these attacks.

### Note: 

This implementation will be done through Tensorflow 2.0 and will not work for any version below 2.0.
